---
Title: Automating update of the Smets and Wouters (2003) database
date: 2015-10-15
Category: Data
Tags: database, model, estimation, R
Slug: sw03-data
Authors: Thomas Brand, Nicolas Toulemonde
Summary: Description step by step to build automatic update of the Smets and Wouters (2003) database.
Download: https://git.nomics.world/macro/sw03-data
output: html_document
---


```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
if (!"pacman" %in% installed.packages()[,"Package"]) install.packages("pacman")
pacman::p_load(tidyverse,rdbnomics,curl,magrittr,lubridate,zoo,mFilter,devtools,knitr,readxl)

opts_chunk$set(message=FALSE, warning=FALSE, cache=FALSE,fig.align="center")

# theme for ggplot
theme <-  theme_bw()+ theme(strip.background=element_blank(),
                            strip.text=element_text(size=15),
                            title=element_text(size=16),
                            panel.border=element_blank(),
                            panel.grid.major=element_line(size=1),
                            legend.key=element_rect(colour="white"),
                            legend.position="bottom",
                            legend.text=element_text(size=10),
                            axis.text=element_text(size=10),
                            plot.title=element_text(hjust=0.5))
blueObsMacro <- "#0D5BA4"
```

Our purpose is to write a program that will automatically update the database used in the bayesian estimation of the DSGE model developped in [@Smet03] for the Euro area. As no dataset for each variable fully covers the whole time period, we need to merge data from different sources.

Eight series are used in the original estimation of [@Smet03] :

* GDP
* GDP Deflator
* Consumption
* Investment
* Employment
* Wage
* Working-age population (15-64)
* Interest rate

To those series we add 3 series :

* Hours worked
* Consumption Deflator
* Investment Deflator

In the original database of [@Smet03], employment is used as a proxy of the hours worked whose series did not exist yet. Hours worked series is now available and we propose to build a long series as explained below.

One main difficulty is the multiplicity of the sources to obtain quarterly data for the Euro area since 1970. Of course, such an aggregation could seem a bit artificial to the extent that the Euro area was highly hypothetical at that time, but papers like [@Smet03] show that it could be interesting to consider it nonetheless. Keeping in mind these limits, we try to obtain one single database by merging data from :

* the Area-Wide Model (AWM), originally constructed by [@Faga01],
* the Conference Board,
* the European Central Bank (ECB),
* Eurostat.

The first three sources are used only for historical data from 1970Q1 to the end of the 1990's. Updates will be fed only with Eurostat data for the eleven series from <a href="https://db.nomics.world/" target="_blank">DBnomics</a>. The <a href="https://api.db.nomics.world/" target="_blank">DBnomics API</a> is used with the <a href="https://cran.r-project.org/web/packages/rdbnomics/index.html" target="_blank">rdbnomics</a> package. All the code is written in R, thanks to the [@Rct16] and [@RStu16]. 

All data are seasonally and working days adjusted, except the interest rate and the population. We also choose to smooth the population.

As explained below, we use an updated version of the AWM database, which includes a Euro area composed of 19 countries. We keep this convention in the definition of the Euro area in the rest of the post.


# Historical data (1970 - end of the 1990's)

Three sources are used to construct the database until the end of the 1990's : the main is the AWM database, but we also use the Conference Board and the ECB databases.

## AWM database

The AWM database was originally developped in [@Faga01]. We use here an updated version of the database available on the  <a href="http://www.eabcn.org/page/area-wide-model" target="_blank">EABCN website</a>. We find here nine of the eleven series mentionned before. The exceptions are the hours worked and the population series, that the AWM database does not include. Those two series come from other sources and wil be treated separately.

```{r}
link_to_awm <- "http://www.eabcn.org/sites/default/files/awm19up15.csv"

if (! "awm19up15.csv" %in% list.files()) {
  download.file(link_to_awm,
                destfile = "awm19up15.csv",
                method = "auto")
}

awm <- read.csv("awm19up15.csv", sep=",")

awm %<>%
  transmute(gdp            = YER, # GDP (Real)
            defgdp         = YED, # GDP Deflator
            conso          = PCR, # Private Consumption (Real)
            defconso       = PCD, # Consumption Deflator
            inves          = ITR, # Gross Investment (Real)
            definves       = ITD, # Gross Investment Deflator
            wage           = WIN, # Compensation to Employees (Nominal)
            shortrate      = STN, # Short-Term Interest Rate (Nominal)
            employ         = LNN, # Total Employment (Persons)
            period         = as.Date(as.yearqtr(X))) %>%
  gather(var, value, -period, convert = TRUE)
```


## First special case : Hours worked

At the time [@Smet03] wrote their paper, hours worked series didn't exist yet, so the authors used a formula linking employment to the hours worked in their model. Now, such a series is provided quarterly by Eurostat since 2000Q1. We propose here to build an historical hours worked series using data from the Conference Board until 1999Q4 and then data from Eurostat. More precisely, historical data come from the <a href="https://www.conference-board.org/data/economydatabase/index.cfm" target="_blank">Total Economy Database</a>.


```{r, fig.height=8.5, fig.width=8}
TED <- "TED---Output-Labor-and-Labor-Productivity-1950-2015.xlsx" 
link_to_confboard <- paste0("https://www.conference-board.org/retrievefile.cfm?filename=",TED,"&type=subsite")

if (! TED %in% list.files()) {
  download.file(link_to_confboard,
                destfile = TED,
                mode="wb")
}

# 19 countries in the Euro area (same as the AWM database)
EAtot_names <- c("Austria", "Belgium", "Cyprus", "Estonia", "Finland", "France", 
                 "Germany", "Greece", "Ireland", "Italy", "Latvia", "Lithuania","Luxembourg", 
                 "Malta", "Netherlands", "Portugal", "Slovak Republic","Slovenia", "Spain")

hours_confboard <- 
  read_excel(TED, "Total Hours Worked", skip=2) %>%
  rename(country=Country)  %>%
  filter(country %in% EAtot_names) %>%
  select(-1) %>%
  gather(period, value, -country, na.rm=TRUE) %>%
  mutate(period = as.Date(paste0(period,"-07-01")),
         value = as.numeric(value)) %>%
  filter(period >= "1970-07-01" & period <= "2012-07-01")

ggplot(hours_confboard,aes(period,value)) +
  geom_line(colour=blueObsMacro) +
  facet_wrap(~country,ncol=4,scales = "free_y") +
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) +
  theme(strip.text=element_text(size=12),
        axis.text=element_text(size=8)) +
  ggtitle("Hours worked")
```

There are still two problems with such a series : first, the series does not cover all the 19 countries inside the Euro area for the whole period; second, data are annual.

### Complete the hours worked series before 1990

Data from 19 countries of the Euro area are available in the Conference Board file only since 1990. 

```{r}
hours_confboard %>% 
  group_by(period) %>% 
  summarize(number_countries = length(country)) %>% 
  tail(n=-12) %>% 
  kable()
```

Indeed, between 1970 and 1990, the hours worked from five countries of Eastern Europe (Estonia, Latvia, Lithuania, Slovak Republic, Slovenia) are missing. So we choose to use the growth rates of the sum of hours worked series for the 14 countries available before 1990 to complete the series of the sum of hours worked over the 19 countries after 1990. It seems legitimate because since 1990, those 14 countries have represented more than 95% of the total hours worked. The `chain` function used here is detailed in the appendix.

```{r chain, echo = FALSE}
chain <- function(to_rebase, basis, date_chain) {
  
  date_chain <- as.Date(date_chain, "%Y-%m-%d")
  
  valref <- basis %>%
    filter(period == date_chain) %>%
    transmute(var, value_ref = value) 
  
  res <- to_rebase %>%
    filter(period <= date_chain) %>%
    arrange(desc(period)) %>%
    group_by(var) %>%
    mutate(growth_rate = c(1, value[-1]/lag(value)[-1])) %>%
    full_join(valref, by = "var") %>%
    group_by(var) %>%
    transmute(period, value = cumprod(growth_rate)*value_ref)%>%
    ungroup() %>% 
    bind_rows(filter(basis, period > date_chain)) %>% 
    arrange(period)
  
  return(res)

}
```


```{r}
# sum over the 14 countries
EA14_names <- c(filter(hours_confboard,period=="1970-07-01")$country)
hours_confboard_14 <- 
  hours_confboard %>%
  filter(country %in% EA14_names) %>% 
  group_by(period) %>% 
  summarize(value = sum(value),
            var = "hours")

# sum over the whole countries
hours_confboard_tot <- 
  hours_confboard %>%
  group_by(period) %>% 
  summarize(value = sum(value),
            var = "hours")

hours_confboard_chained <- 
  chain(to_rebase = hours_confboard_14, 
        basis = hours_confboard_tot, 
        date_chain = "1990-07-01")
```


### Convert the annual hours worked series to quarterly data before 2000

Once the annual data have been completed since 1970, they have to be turned into quarterly data. 

```{r}
hours_confboard_chained_q <- 
  tibble(period=seq(as.Date("1970-07-01"),
                    as.Date("2012-07-01"),
                    by = "quarter"),
         value=NA) %>% 
  left_join(hours_confboard_chained,by="period") %>% 
  select(-value.x) %>% 
  rename(value=value.y)
```

Several methods of interpolation are tested : 

* constant quarterly growth rate over one year 
* cubic spline 
* Kalman filter

```{r}
hours <- hours_confboard_chained_q

hours_approx <- 
  hours %>% 
  mutate(value=na.approx(value),
         var="hours_approx")

hours_spline <- 
  hours %>% 
  mutate(value=na.spline(value),
         var="hours_spline")

hoursts <- ts(hours$value,start=c(1970,4),f=4)
smoothed_hoursts <- tsSmooth(StructTS(hoursts,type="trend"))[,1]

hours_StructTS <- 
  hours %>% 
  mutate(value=smoothed_hoursts,
         var="hours_kalman")

hours_filtered <- bind_rows(hours_approx,hours_spline,hours_StructTS)

hours_filtered_levgr <- 
  hours_filtered %>% 
  mutate(value=log(value)-log(lag(value))) %>% 
  data.frame(ind2="2- Growth rates") %>% 
  bind_rows(data.frame(hours_filtered,ind2="1- Levels")) %>% 
  filter(period>="1971-01-01")

ggplot(hours_filtered_levgr,aes(period,value,colour=var))+
  geom_line()+
  facet_wrap(~ind2,scales = "free_y",ncol = 1)+
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) + 
  theme(legend.title=element_blank()) +
  ggtitle("Hours")
```

We retain the Kalman filter method of interpolation because we want to avoid the jump each first quarter in the growth rate implied by the first method, and the high volatility implied by the third filtering.
```{r}
hours <- hours_StructTS %>% 
  mutate(var="hours")

```
### Compare the different series of hours worked

We want to check graphically that the interpolation of annual hours worked with the Kalman filter is consistent with raw data available in the most recent period. Remind that the interpolation is used only before 2000, we present interpolated data after this date in the plots only to check the consistency of our filtering, but will not use them after. To get recent Eurostat data, we use a [plugin function](https://cran.r-project.org/web/packages/rdbnomics/index.html) from  [DBnomics](https://db.nomics.world/).

```{r}
# convert Conference board annual hours worked series in 2000 basis index
valref <- filter(hours_confboard_chained,period=="2000-07-01")$value
hoursconfboard_ind <- 
  hours_confboard_chained %>% 
  transmute(period=period,
            var="Annual hours (original, Conference board)",
            value=value/valref)

# Quarterly hours worked series from Eurostat
df <- rdb(ids = "Eurostat/namq_10_a10_e/Q.THS_HW.TOTAL.SCA.EMP_DC.EA19")
eurostat_data <-
  df %>% 
  select(period,value) %>% 
  mutate(var = "Quarterly hours (original, Eurostat)")

valref <- 
  eurostat_data %>% 
  filter(year(period)==2000) %>% 
  summarize(value=mean(value))

eurostat_data_ind <- 
  eurostat_data %>% 
   mutate(value=value/valref$value)

# convert interpolated hours worked series in 2000 basis index
valref <- 
  hours %>% 
  filter(year(period)==2000) %>% 
  summarize(value=mean(value))

hours_ind <- 
  hours %>% 
  transmute(period,
            var="Quarterly hours (interpolated)",
            value=value/valref$value)


check <- bind_rows(hoursconfboard_ind,
                   eurostat_data_ind,
                   hours_ind)

ggplot(check, aes(period, value, group = var, linetype = var, colour = var)) + 
  geom_line() + 
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) + 
  theme(legend.title=element_blank()) +
  guides(col=guide_legend(ncol=1),lty=guide_legend(ncol=1)) +
  ggtitle("Comparison of hours worked series")
```


## Second special case : Population

The second special case is the population series. Quarterly population series only exist for the Euro Area after 2005. We propose here to construct an historical population series using annual data from Eurostat by country until 2005 and then original Euro area quarterly population series from Eurostat through DBnomics. 

```{r, fig.height=8.5, fig.width=8}
# We build the URL for the DBnomics API to get annual population series for the 19 countries of the Euro Area
EAtot_code <- c("AT", "BE", "CY", "DE_TOT", "EE", "IE", 
                "EL", "ES","FX", "IT", "LT","LV", "LU", 
                "NL", "PT", "SK", "FI", "MT", "SI")
url_country <- paste0("A.NR.Y15-64.T.",paste0(EAtot_code, collapse = "+"))
df <- rdb("Eurostat","demo_pjanbroad",mask = url_country)

pop_eurostat_bycountry <-
  df %>% 
  select(geo, period, value) %>% 
  rename(country = geo) %>% 
   filter(period >= "1970-01-01", 
         period <= "2013-01-01")

plot_pop_eurostat_bycountry <-
  pop_eurostat_bycountry %>% 
  mutate(value = value/1000000)

ggplot(plot_pop_eurostat_bycountry,aes(period,value)) +
  geom_line(colour=blueObsMacro) +
  facet_wrap(~country,ncol=4,scales = "free_y") +
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) +
  theme(strip.text=element_text(size=12),
        axis.text=element_text(size=8)) +
  ggtitle("Population 15-64 (in millions)")
```

There are still two problems with such a series : first, the series does not cover all the 19 countries inside the Euro area for the whole period; second, data are annual.

### Complete the population series before 1982

Between 1970 and 1981 (included), Eurostat only provides the population for 16 countries in the Euro area (not for Malta, Slovenia, and Cyprus). 

```{r}
pop_eurostat_bycountry %>%
  group_by(period) %>% 
  summarize(number_countries = length(country)) %>% 
  kable()
```

As those 16 countries represent more than 95% of the population of the EA19 in the last decades, we chain the series of the sum of the population for 19 countries to the series of the sum for 16 countries between 1970 and 1982. The method is the same as the one used with the series of hours worked: we use the `chain` function detailed in the appendix.

```{r}
# We sum the annual population for 16 countries in the Euro area
EA16_code <- filter(pop_eurostat_bycountry,period=="1970-01-01")$country
pop_a_16 <- 
  pop_eurostat_bycountry %>% 
  filter(country %in% EA16_code) %>% 
  group_by(period) %>% 
  summarize(value = sum(value),
            var = "pop")

# We sum the annual population for all the available countries
pop_a_tot <- 
  pop_eurostat_bycountry %>%
  group_by(period) %>% 
  summarize(value = sum(value),
            var="pop")

# We use the chain function detailed in the appendix
pop_chained <- 
  chain(to_rebase = pop_a_16, 
        basis = pop_a_tot, 
        date_chain = "1982-01-01")
```

### Convert the annual population series to quarterly data before 2000

Once the annual data have been completed since 1970, they have to be turned into quarterly data. 

```{r}
pop_chained_q <- 
  tibble(period=seq(as.Date("1970-01-01"),
                    as.Date("2013-01-01"),
                    by = "quarter"),
         value=NA) %>% 
  left_join(pop_chained, by="period") %>% 
  select(-value.x) %>% 
  rename(value=value.y)
```

We test the same three methods of interpolation as for hours : 

* constant quarterly growth rate over one year 
* cubic spline 
* Kalman filter

```{r}
pop <- pop_chained_q

pop_approx <- 
  pop %>% 
  mutate(value=na.approx(value),
         var="pop_approx")

pop_spline <- 
  pop %>% 
  mutate(value=na.spline(value),
         var="pop_spline")

popts <- ts(pop$value,start=c(1970,1),f=4)
smoothed_popts <- tsSmooth(StructTS(popts,type="trend"))[,1]

pop_StructTS <- 
  pop %>% 
  mutate(value=smoothed_popts,
         var="pop_kalman")

pop_filtered <- bind_rows(pop_approx,pop_spline,pop_StructTS)

pop_filtered_levgr <- pop_filtered %>% 
  mutate(value=log(value)-log(lag(value))) %>% 
  data.frame(ind2="2- Growth rates") %>% 
  bind_rows(data.frame(pop_filtered,ind2="1- Levels")) %>% 
  filter(period>="1970-04-01")

ggplot(pop_filtered_levgr,aes(period,value,colour=var))+
  geom_line()+
  facet_wrap(~ind2,scales = "free_y",ncol = 1)+
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) + 
  theme(legend.title=element_blank()) +
  ggtitle("Population")
```


We retain the Kalman filter method of interpolation to avoid the jump each first quarter in the growth rate implied by the first method, and the high volatility implied by the third filtering.

```{r}
pop <- pop_StructTS %>% 
  mutate(var="pop")
```

### Compare the different series of population

We want to check graphically that the interpolation of annual population with the Kalman filter is consistent with raw data available in the most recent period. Remind that the interpolation is used only before 2005, we present interpolated data after this date in the plots only to check the consistency of our filtering, but will not use them after. 


```{r}
# convert Conference board annual hours worked series in 2000 basis index
valref <- filter(pop_chained,period=="2005-01-01")$value
pop_a_ind <- 
  pop_chained %>% 
  transmute(period=period,
            var="Annual population (original, Eurostat)",
            value=value/valref)

# URL for quarterly population series
df <- rdb(ids="Eurostat/lfsq_pganws/Q.THS.T.TOTAL.Y15-64.POP.EA19")

eurostat_data <- 
  df %>%
  select(period, geo, value) %>%
  rename(var= geo) %>%
  mutate(var= "Quarterly population (orginal, Eurostat)") %>%
  filter(period >= "2005-01-01")

valref <- 
  eurostat_data%>% 
  filter(year(period)==2005) %>% 
  summarize(value=mean(value))

eurostat_data_ind <- 
  eurostat_data %>% 
  mutate(value=value/valref$value)

# convert interpolated population series in 2000 basis index
valref <- 
  pop %>% 
  filter(year(period)==2005) %>% 
  summarize(value=mean(value))

pop_ind <- 
  pop %>% 
  transmute(period,
            var="Quarterly population (interpolated)",
            value=value/valref$value)

check <- bind_rows(pop_a_ind,
                   eurostat_data_ind,
                   pop_ind)

ggplot(check, aes(period, value,colour=var))+
  geom_line() + 
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) + 
  theme(legend.title=element_blank()) +
  guides(col=guide_legend(ncol=1),lty=guide_legend(ncol=1)) +
  ggtitle("Population")
```


# Recent data (since the end of the 1990's)

Once the historical database is created, all the variables can be found in Eurostat without transformation, through DBnomics.

```{r}
old_data <- bind_rows(awm,
                      hours,
                      pop)

# URL for GDP/Consumption/Investment volumes and prices data
variable.list <- c("B1GQ","P31_S14_S15","P51G")
measure.list <- c("CLV10_MEUR","PD10_EUR")
url_var <- paste0(variable.list,collapse = "+")
url_meas <- paste0(measure.list,collapse = "+")
filter <- paste0("Q.",url_meas,".SCA.", url_var, ".EA19")
df <- rdb("Eurostat","namq_10_gdp",mask = filter)

d1 <-
  df %>%
  select(period, value,unit, na_item,series_name) %>% 
  rename(var = na_item) %>% 
  mutate( var = ifelse(var=="B1GQ"&unit=="CLV10_MEUR","gdp",
                       ifelse(var=="B1GQ","defgdp",
                              ifelse(var=="P31_S14_S15"&unit=="CLV10_MEUR","conso",
                                     ifelse(var=="P31_S14_S15","defconso",
                                            ifelse(var=="P51G"&unit=="CLV10_MEUR","inves","definves")))))) %>%
  transmute(period,var,value,series_name)

# URL for wage series
df <- rdb(ids="Eurostat/namq_10_a10/Q.CP_MEUR.SCA.TOTAL.D1.EA19")

d2 <- 
  df %>%
  select(period, unit, value, series_name) %>%
  rename(var=unit) %>%
  mutate(var="wage")

# URL for hours and employement
url_meas <- "THS_HW+THS_PER"
filter <- paste0("Q.",url_meas,".TOTAL.SCA.EMP_DC.EA19")
df <- rdb("Eurostat","namq_10_a10_e",mask = filter)

d3 <- 
  df %>%
  select(period, unit, value, series_name) %>%
  rename(var= unit) %>%
  mutate(var=ifelse(var=="THS_HW","hours","employ")) %>% 
  transmute(period,var,value, series_name)

# URL for quarterly 3-month rates
df <- rdb(ids="Eurostat/irt_st_q/Q.IRT_M3.EA")

d4 <- 
  df %>%
  select(period, geo, value, series_name) %>%
  rename(var= geo) %>%
  mutate(var= "shortrate")

# URL for quarterly population series
df <- rdb(ids="Eurostat/lfsq_pganws/Q.THS.T.TOTAL.Y15-64.POP.EA19")

d5 <- 
  df %>%
  select(period, geo, value, series_name) %>%
  rename(var= geo) %>%
  mutate(var= "pop") %>%
  filter(period >= "2005-01-01")

recent_data <- bind_rows(d1,d2,d3,d4,d5)
```

```{r echo=FALSE}
var_names <- unique(recent_data$series_name)
recent_data <- 
  recent_data %>%
  select(-series_name)
```

We can check the last date available for each variable. 
```{r}
maxDate <- 
  recent_data %>% 
  group_by(var) %>% 
  summarize(maxdate=max(period)) %>% 
  arrange(maxdate)
kable(maxDate)
```


```{r}
minmaxDate <- min(maxDate$maxdate)
recent_data %<>% filter(period <= minmaxDate)
```

Then we filter the recent database until `r as.yearqtr(minmaxDate)`.

# Final database

Now we can create the final database and chain the 11 historical series on the 11 recent series. To chain those series, we keep unchanged recent data from Eurostat and rebase the historical data. 

We can check the first date available for each variable in the recent database.
```{r}
minDate <- 
  recent_data %>% 
  group_by(var) %>% 
  summarize(maxdate=min(period)) %>% 
  arrange(maxdate)
kable(minDate)
```

All the variables except the population (GDP, consumption, investment, their deflators, interest rates, hours, wage and employment) are chained at 1999Q1, official date of the creation of the Euro area.
```{r}
vars <- c("gdp","conso","inves","defgdp","defconso","definves","shortrate", "hours", "wage", "employ")
new_df <- 
  recent_data %>%
  filter(var %in% vars)
old_df <- 
  awm %>%
  filter(var %in% vars) %>%
  bind_rows(hours)
df1 <- chain(basis = new_df,
             to_rebase = old_df,
             date_chain = "1999-01-01")
```

## Population special case

### Chain and smooth the population series

Population is a special case because we need to chain recent data with the historical series in 2005 (beginning of the population quarterly series) and we also need to make sure the series is as smooth as possible for normalization. First we chain the two series.

```{r}
recent_pop_q <- filter(recent_data, var == "pop")

minDatePopQ <- min(recent_pop_q$period)

pop <- chain(basis = recent_pop_q,
             to_rebase= pop,
             date_chain=minDatePopQ)

plot_pop <- pop %>% 
  mutate(value=log(value)-log(lag(value))) %>% 
  data.frame(ind2="Growth rates") %>% 
  filter(period>="1970-04-01")

ggplot(plot_pop, aes(period, value, colour = var)) +
  geom_line() +
  facet_wrap(~ind2,scales = "free_y",ncol = 1)+
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) + 
  theme(legend.title=element_blank()) +
  ggtitle("Quarterly Population")
```

The last years of the series exhibits a high level of volatility because they were not interpolated via a Kalman filter, we thus apply a Hodrick-Prescott filter to the series.

```{r}
popts <- ts(pop$value,start=c(1970,1),f=4)
smoothed_popts <- hpfilter(popts, freq=1600)$trend
  
pop_StructTS <- 
  pop %>% 
  mutate(value=as.numeric(smoothed_popts),
         var="Smoothed population")
plot_pop <-
  pop %>%
  mutate(var="Original population")

pop_filtered <- bind_rows(plot_pop, pop_StructTS)

pop_filtered_levgr <- pop_filtered %>% 
  mutate(value=log(value)-log(lag(value))) %>% 
  data.frame(ind2="2- Growth rates") %>% 
  bind_rows(data.frame(pop_filtered,ind2="1- Levels")) %>% 
  filter(period>="1970-04-01")

ggplot(pop_filtered_levgr,aes(period,value,colour=var))+
  geom_line()+
  facet_wrap(~ind2,scales = "free_y",ncol=1)+
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) + 
  theme(legend.title=element_blank()) +
  ggtitle("Population")
```

We retain the smoothed serie with the Hodrick-Prescott filter.

```{r}
pop <- pop_StructTS %>%
  mutate(var = "pop")
```

So we can produce the final update of the Smets and Wouters (2003) database.

```{r, fig.height=8.5, fig.width=8}
final_df <- bind_rows(df1, pop)

plot_df <- final_df
listVar <- list("Real GDP [1]" = "gdp",
                "Real consumption [2]" = "conso",
                "Real investment [3]" = "inves",
                "GDP deflator [4]" = "defgdp",
                "Consumption deflator [5]" = "defconso",
                "Investment deflator [6]" = "definves",
                "Real wage [7]" = "wage",
                "Hours worked [8]"= "hours",
                "Employment [9]" = "employ",
                "Interest rate [10]" = "shortrate",
                "Population [11]" = "pop")

plot_df$var <- factor(plot_df$var)
levels(plot_df$var)<-listVar
                     
ggplot(plot_df,aes(period,value))+
  geom_line(colour=blueObsMacro)+
  facet_wrap(~var,scales = "free_y",ncol = 3)+
  scale_x_date(expand = c(0.01,0.01)) +
  theme + xlab(NULL) + ylab(NULL) +
  theme(strip.text=element_text(size=12),
        axis.text=element_text(size=7))
```
```{r, echo=F}
var_names
```

You can download the 11 series directly <a href="http://shiny.cepremap.fr/data/EA_SW_rawdata.csv" target="_blank">here</a>.
```{r}
EA_SW_rawdata <-
  final_df %>%
  spread(key = var, value= value)

EA_SW_rawdata %>%
  write.csv("EA_SW_rawdata.csv", row.names=FALSE)
```

You can also download ready-to-use (normalized) data for the estimation <a href="http://shiny.cepremap.fr/data/EA_SW_data.csv" target="_blank">here</a>.
```{r}
EA_SW_data <-
  final_df %>% 
  mutate(period=gsub(" ","",as.yearqtr(period))) %>%
  spread(key = var, value = value) %>%
  transmute(period = period,
            gdp_rpc=1e+6*gdp/(pop*1000),
            conso_rpc=1e+6*conso/(pop*1000),
            inves_rpc=1e+6*inves/(pop*1000),
            defgdp=defgdp,
            wage_rph=1e+6*wage/defgdp/(hours*1000),
            hours_pc=1000*hours/(pop*1000),
            pinves_defl=definves/defgdp,
            pconso_defl=defconso/defgdp,
            shortrate=shortrate/100,
            employ=1000*employ/(pop*1000))

EA_SW_data %>% 
  na.omit() %>%
  write.csv("EA_SW_data.csv", row.names=FALSE)
```


# Appendix

## Chaining function
To chain two datasets, we build a chain function whose input must be two dataframes with three standard columns (`period`, `var`, `value`). It returns a dataframe composed of chained values, ie the dataframe "to rebase" will be chained on the "basis" dataframe. 

More specifically, the function : 

* computes the growth rates from `value` in the dataframe of the 1st argument 
* multiplies it with the value of reference chosen in `value` in the dataframe of the 2nd argument 
* at the `date` specified in the 3rd argument.

```{r chain}
```
